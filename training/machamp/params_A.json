{
  "transformer_model": "lct-rug-2022/dont_stop_pretraining-roberta-large-2M-basic_preproc",
  "random_seed": 8446,
  "default_dec_dataset_embeds_dim": 12,
  "encoder": {
    "dropout": 0.2,
    "max_input_length": 512,
    "update_weights_encoder": true
  },
  "decoders": {
    "default_decoder": {
      "loss_weight": 1.0,
      "metric": "f1_macro",
      "topn": 1,
      "layers_to_use": [-1]
    },
    "classification": {}  // removed all other tasks for our case specifically
  },
  "batching": {
    "max_tokens": 1024,
    "batch_size": 4,
    "sort_by_size": true,
    "sampling_smoothing": 1.0 // 1.0 == original size, 0.0==all equal
  },
  "training": {
    "keep_top_n": 1,
    "learning_rate_scheduler": {
      //"type": "slanted_triangular",
      "cut_frac": 0.3,
      "decay_factor": 0.38,
      "discriminative_fine_tuning": false,
      "gradual_unfreezing": false
    },
    "num_epochs": 20,
    "optimizer": {
      //"type": "adamw",
      "betas": [
        0.9,
        0.99
      ],
      "lr": 0.000005,
      "correct_bias": false,
      //"patience": 5, // disabled, because slanted_triangular changes the lr dynamically
      "weight_decay": 0.01
    }
  }
}
